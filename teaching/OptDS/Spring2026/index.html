<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Nicolas Loizou</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26204762-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>


  </head>


  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <ul class="menu">
            <li><a href="index.html">About</a></li>
            <li><a href="news.html"> News </a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="team.html">Team</a></li>
            <li><a href="talks.html">Talks</a></li>
            <li><a href="bio.html">Bio</a></li>
            <li><a href="ProfServ.html">Professional Services</a></li>
            <li><a class="active" href="teaching.html">Teaching</a></li>
            <li><a href="openpositions.html">Open Positions</a></li>
          </ul>
  <br>
          <script src="table_header.js"></script> </div>

      <div id="wrapper" class="main">
        <div id="content">
<br>
<br>
<br>
<br>

<style>
  .section-title {
    font-size: 1.35em;
    font-weight: 800;
    margin: 18px 0 8px 0;
    padding-bottom: 6px;
    border-bottom: 2px solid #d9d9d9;
    letter-spacing: 0.2px;
  }
</style>

<h1>EN.553.662: Optimization for Data Science</h1>
<hr>

<p>
  <b>Previous offerings:</b>
  <a href="https://nicolasloizou.github.io/teaching.html">Fall 2024</a> ·
  <a href="https://nicolasloizou.github.io/teaching.html">Fall 2023</a>
</p>

<h2>Spring 2026</h2>

<div class="section-title">Course Information</div>

<div style="margin-top: 12px; margin-bottom: 14px;">
  <p style="margin: 4px 0;"><b>Instructor:</b> Nicolas Loizou</p>
  <p style="margin: 4px 0;"><b>Lecture times:</b> Monday, Wednesday 12:00 pm – 01:15 pm (01/20 – 04/27)</p>
  <p style="margin: 4px 0;"><b>Location:</b> Homewood Campus, Bloomberg 274</p>
  <p style="margin: 4px 0;"><b>Syllabus (PDF):</b>
    <a href="./Syllabus_Optimization%20for%20Data%20Science_Spring2026.pdf">Download</a>
  </p>
</div>

<h3>Teaching Assistants &amp; Office Hours</h3>
<ul>
  <li>Ben Weinberg (<a href="mailto:bweinbe5@jhu.edu">bweinbe5@jhu.edu</a>) — Tue 3:00 pm – 5:00 pm</li>
  <li>Yuxin Ma (<a href="mailto:yma93@jh.edu">yma93@jh.edu</a>) — Tue 10:30 am – 12:30 pm</li>
  <li>Qi Li (<a href="mailto:qli112@jh.edu">qli112@jh.edu</a>) — Thu 3:00 pm – 5:00 pm</li>
</ul>

<b>Course Description</b>
<p>
Optimization formulations and algorithms play a central role in data analysis and machine learning.
In the era of big data, the need to solve large-scale optimization problems is ubiquitous across industry
and science. This course is a mathematically rigorous introduction to large-scale optimization for data
science and machine learning, emphasizing algorithms, convergence/complexity guarantees, and practical
implementation. Applications include text analysis, page ranking, speech recognition, image classification,
finance, and decision sciences.
</p>

<b>Prerequisites / Corequisites</b>
<ul>
  <li>Linear algebra (vector spaces, quadratic forms, inner product, norms, etc.)</li>
  <li>Multivariate calculus (gradient, Hessian, Taylor approximation, chain rule, etc.)</li>
  <li>Probability theory (expectation, law of large numbers, tower property, etc.)</li>
  <li>Numerical programming (MATLAB / Python / Julia or equivalent)</li>
</ul>
<p>
Some degree of mathematical maturity (ability to read/write proofs) is required. Background in optimization
theory is highly recommended.
</p>

<b>Course Materials</b>
<ul>
  <li>Detailed slides (posted before each lecture)</li>

  <li>Primary resources:
    <ul>
      <li>
        <i>Optimization for Data Analysis</i> (Ch. 1–5), Stephen J. Wright &amp; Benjamin Recht
        <br>
        <span style="color:#555;">
          Publisher page (not open access):
          <a href="https://www.cambridge.org/core/books/optimization-for-data-analysis/C02C3708905D236AA354D1CE1739A6A2" target="_blank" rel="noopener">Cambridge Core</a>.
          &nbsp;Free alternative (author-posted notes):
          <a href="https://raw.githubusercontent.com/wrightstephen/TRIPODS-EDUCATION/master/TEXT/Wright/Wright-Optimization-Algorithms-For-Data-Analysis.pdf" target="_blank" rel="noopener">Wright: Optimization Algorithms for Data Analysis (PDF)</a>.
        </span>
      </li>

      <li>
       <i>First-order and Stochastic Optimization Methods for Machine Learning</i> (Ch. 1–6), Guanghui Lan
        <br>
        <span style="color:#555;">
          Open-access draft (author):
          <a href="https://sites.gatech.edu/guanghui-lan/wp-content/uploads/sites/330/2019/08/LectureOPTML.pdf" target="_blank" rel="noopener">Lectures on Optimization Methods for Machine Learning (PDF)</a>.
          &nbsp;Publisher page:
          <a href="https://link.springer.com/book/10.1007/978-3-030-39568-1" target="_blank" rel="noopener">SpringerLink</a>.
        </span>
      </li>

    </ul>
  </li>

  <li>Further useful references:
    <ul>
      <li>
         <i>Convex Optimization: Algorithms and Complexity</i>, Sébastien Bubeck
        <br>
        <span style="color:#555;">
          Open access:
          <a href="https://sbubeck.com/Bubeck15.pdf" target="_blank" rel="noopener">PDF (author site)</a>.
        </span>
      </li>

      <li>
     <i>First-Order Methods in Optimization</i>, Amir Beck
        <br>
        <span style="color:#555;">
          Publisher page (not open access):
          <a href="https://www.siam.org/publications/siam-news/articles/first-order-methods-in-optimization/" target="_blank" rel="noopener">SIAM</a>.
        </span>
      </li>

      <li>
        <i>Understanding Machine Learning: From Theory to Algorithms</i>,  S Shalev-Schwartz, &amp;  S Ben-David
        <br>
        <span style="color:#555;">
          Open access (authors):
          <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf" target="_blank" rel="noopener">PDF</a>.
        </span>
      </li>
      <li>
         <i>Advances and Open Problems in Federated Learning</i> (Ch. 1–3) Peter Kairouz et al.
        <br>
        <span style="color:#555;">
          Open access:
          <a href="https://arxiv.org/pdf/1912.04977.pdf" target="_blank" rel="noopener">arXiv PDF</a>.
        </span>
      </li>
    </ul>
  </li>
</ul>

<b>Learning Outcomes</b>
<ul>
  <li>Analyze scalable nonlinear optimization algorithms used in ML and data science.</li>
  <li>Execute high-performance computing (HPC) implementations of optimization algorithms in selected applications.</li>
  <li>Select algorithms based on problem structure (convex/nonconvex, smooth/nonsmooth, data regime, memory/distributed constraints).</li>
</ul>

<b>Evaluation</b>
<ul>
  <li>Assignments: 50%</li>
  <li>Exams: 50%</li>
</ul>

<style>
  /* Tighter spacing for the outline block */
  .section-title{
    font-size: 1.35em;
    font-weight: 800;
    margin: 14px 0 6px 0;          /* tighter */
    padding-bottom: 5px;
    border-bottom: 2px solid #d9d9d9;
    letter-spacing: 0.2px;
  }

  .outline-part{
    margin: 10px 0 6px 0;          /* tighter */
    display: flex;
    align-items: baseline;
    gap: 10px;
  }

  .outline-badge{
    display: inline-block;
    padding: 2px 9px;
    border: 1px solid #e0e0e0;
    border-radius: 999px;
    background: #fafafa;
    font-weight: 600;
    font-size: 0.92em;
    color: #333;
    white-space: nowrap;
  }

  .outline-title{
    font-size: 1.08em;
    font-weight: 700;
    margin: 0;                    /* IMPORTANT: remove default <p> margins */
  }

  ol.outline{
    margin: 6px 0 10px 0;         /* tighter */
    padding-left: 22px;
  }

  ol.outline > li{
    margin: 8px 0 10px 0;         /* tighter */
  }

  .topic{
    font-weight: 700;
    font-size: 1.02em;
    margin: 0 0 2px 0;            /* tighter */
  }

  .focus{
    color: #555;
    font-size: 0.95em;
    margin: 2px 0 4px 0;          /* tighter */
  }

  .subtopics{
    margin: 4px 0 0 0;            /* tighter */
    padding-left: 18px;
  }

  .subtopics li{
    margin: 2px 0;                /* tighter */
  }
</style>

<div class="section-title">Course Outline</div>

<div class="outline-part">
  <span class="outline-badge">Part I</span>
  <p class="outline-title">Stochastic Optimization</p>
</div>

<ol class="outline">
  <li>
    <div class="topic">Foundations of Large-Scale Optimization</div>
    <div class="focus">Core setup + tools; deterministic GD as baseline.</div>
    <ul class="subtopics">
      <li>Stochastic optimization for machine learning</li>
      <li>Basic tools from convex analysis, optimization, and probability</li>
      <li>Deterministic gradient descent</li>
    </ul>
  </li>

  <li>
    <div class="topic">Stochastic Gradient Descent &amp; Variants</div>
    <div class="focus">Step-size policies + guarantees across regimes.</div>
    <ul class="subtopics">
      <li>Assumptions</li>
      <li>Constant step-size: strongly convex, convex, nonconvex guarantees</li>
      <li>Decreasing step-size: strongly convex, convex, nonconvex guarantees</li>
      <li>Mini-batches, sampling strategies (uniform vs. importance sampling)</li>
    </ul>
  </li>

  <li>
    <div class="topic">Variance Reduction Methods</div>
    <div class="focus">Faster convergence via control variates.</div>
    <ul class="subtopics">
      <li>SVRG, L-SVRG, SAGA</li>
      <li>Unified analysis of stochastic gradient methods</li>
    </ul>
  </li>

  <li>
    <div class="topic">Linear System Solvers</div>
    <div class="focus">Randomized solvers and links to SGD.</div>
    <ul class="subtopics">
      <li>Sketch-and-project methods</li>
      <li>Randomized Kaczmarz; randomized coordinate descent (Gauss–Seidel)</li>
      <li>Stochastic reformulations and connections to SGD</li>
    </ul>
  </li>

  <li>
    <div class="topic">Acceleration with Momentum</div>
    <div class="focus">Heavy-ball and Nesterov; stochastic momentum variants.</div>
    <ul class="subtopics">
      <li>Heavy-ball (Polyak) momentum</li>
      <li>Nesterov acceleration</li>
      <li>Momentum variants in stochastic settings</li>
    </ul>
  </li>

  <li>
    <div class="topic">Adaptive / Parameter-Free Methods</div>
    <div class="focus">Adaptive scaling and parameter-free step sizes.</div>
    <ul class="subtopics">
      <li>AdaGrad, RMSProp, Adam</li>
      <li>Stochastic Polyak step-size</li>
    </ul>
  </li>
</ol>

<div class="outline-part">
  <span class="outline-badge">Part II</span>
  <p class="outline-title">Variational Inequalities &amp; Min-Max Optimization</p>
</div>

<ol class="outline" start="7">
  <li>
    <div class="topic">Background</div>
    <div class="focus">Formulations, assumptions, and ML connections.</div>
    <ul class="subtopics">
      <li>Formulation and connections to optimization</li>
      <li>Variational inequalities, min-max optimization, smooth games, saddle points</li>
      <li>Applications in ML; key assumptions</li>
    </ul>
  </li>

  <li>
    <div class="topic">Popular Algorithms</div>
    <div class="focus">GDA, extragradient, and game-optimization methods.</div>
    <ul class="subtopics">
      <li>Gradient descent–ascent (and stochastic variants)</li>
      <li>Extragradient methods (and stochastic variants)</li>
      <li>Hamiltonian gradient methods and consensus optimization</li>
    </ul>
  </li>
</ol>

<div class="outline-part">
  <span class="outline-badge">Part III</span>
  <p class="outline-title">Distributed / Decentralized / Federated Optimization</p>
</div>

<ol class="outline" start="9">
  <li>
    <div class="topic">Main Algorithms</div>
    <div class="focus">Why distribution matters; core methods and settings.</div>
    <ul class="subtopics">
      <li>Motivation &amp; applications</li>
      <li>Distributed &amp; decentralized methods</li>
    </ul>
  </li>

  <li>
    <div class="topic">Rules for Improved Communication Complexity</div>
    <div class="focus">Communication efficiency via compression and locality.</div>
    <ul class="subtopics">
      <li>Compressed operators</li>
      <li>Local updates</li>
      <li>Network topology</li>
    </ul>
  </li>
</ol>



<div class="section-title">Schedule</div>
<p style="margin: 6px 0 10px 0; color:#555; font-size:0.95em;">
  The links in the table below will be updated as the semester progresses.
</p>
<style>
  /* Lightweight, elegant table styling (scoped by class) */
  table.course_schedule {
    width: 100%;
    border-collapse: collapse;
    margin-top: 10px;
    margin-bottom: 18px;
    font-size: 0.98em;
  }
  table.course_schedule th, table.course_schedule td {
    border-bottom: 1px solid #e6e6e6;
    padding: 10px 10px;
    vertical-align: top;
  }
  table.course_schedule th {
    text-align: left;
    border-bottom: 2px solid #d9d9d9;
    background: #fafafa;
    font-weight: 600;
  }
  table.course_schedule td.links a {
    margin-right: 10px;
    white-space: nowrap;
  }
  table.course_schedule tr:hover td {
    background: #fcfcfc;
  }
  .tag {
    display: inline-block;
    padding: 2px 8px;
    border: 1px solid #e0e0e0;
    border-radius: 999px;
    font-size: 0.85em;
    margin-left: 6px;
    color: #444;
    background: #fff;
  }
</style>

<table class="course_schedule">
  <thead>
    <tr>
      <th style="width: 14%;">Lectures</th>
      <th style="width: 58%;">Topic</th>
      <th style="width: 28%;">Materials</th>
    </tr>
  </thead>
  <tbody>
    <!-- ====== Grouped lecture blocks (edit freely) ====== -->

    <tr>
      <td>L1–L3</td>
      <td>
        Foundations of Large-Scale Optimization
        <span class="tag">Part I</span><br>
        <span style="color:#555;">Stochastic optimization for ML; convex/probability tools; deterministic gradient descent.</span>
      </td>
      <td class="links">
        <a href="slides/L01-L03.pdf">Slides</a>
        <a href="notes/L01-L03.pdf">Notes</a>
      </td>
    </tr>

    <tr>
      <td>L4–L7</td>
      <td>
        Stochastic Gradient Descent &amp; Variants
        <span class="tag">Part I</span><br>
        <span style="color:#555;">Assumptions; step-size policies; convex/strongly convex/nonconvex guarantees; mini-batching and sampling.</span>
      </td>
      <td class="links">
        <a href="slides/L04-L07.pdf">Slides</a>
        <a href="notes/L04-L07.pdf">Notes</a>
      </td>
    </tr>

    <tr>
      <td>L8–L9</td>
      <td>
        Variance Reduction Methods
        <span class="tag">Part I</span><br>
        <span style="color:#555;">SVRG, L-SVRG, SAGA; unified analysis of stochastic gradient methods.</span>
      </td>
      <td class="links">
        <a href="slides/L08-L09.pdf">Slides</a>
        <a href="notes/L08-L09.pdf">Notes</a>
      </td>
    </tr>

    <tr>
      <td>L10</td>
      <td>
        Linear System Solvers
        <span class="tag">Part I</span><br>
        <span style="color:#555;">Sketch-and-project methods; stochastic reformulations and connections to SGD.</span>
      </td>
      <td class="links">
        <a href="slides/L10.pdf">Slides</a>
        <a href="notes/L10.pdf">Notes</a>
      </td>
    </tr>

    <tr>
      <td>L11–L12</td>
      <td>
        Randomized Kaczmarz &amp; Coordinate Descent
        <span class="tag">Part I</span><br>
        <span style="color:#555;">Randomized Kaczmarz; randomized coordinate descent (Gauss–Seidel) for linear systems.</span>
      </td>
      <td class="links">
        <a href="slides/L11-L12.pdf">Slides</a>
        <a href="notes/L11-L12.pdf">Notes</a>
      </td>
    </tr>

    <tr>
      <td>L13–L14</td>
      <td>
        Acceleration with Momentum
        <span class="tag">Part I</span><br>
        <span style="color:#555;">Heavy-ball (Polyak) momentum; Nesterov acceleration; stochastic momentum variants.</span>
      </td>
      <td class="links">
        <a href="slides/L13-L14.pdf">Slides</a>
        <a href="notes/L13-L14.pdf">Notes</a>
      </td>
    </tr>

    <tr>
      <td>L15–L16</td>
      <td>
        Adaptive / Parameter-Free Methods
        <span class="tag">Part I</span><br>
        <span style="color:#555;">AdaGrad, RMSProp, Adam; stochastic Polyak step-size.</span>
      </td>
      <td class="links">
        <a href="slides/L15-L16.pdf">Slides</a>
        <a href="notes/L15-L16.pdf">Notes</a>
      </td>
    </tr>

    <tr>
      <td>L17–L19</td>
      <td>
        Variational Inequalities &amp; Min-Max Optimization — Background
        <span class="tag">Part II</span><br>
        <span style="color:#555;">Formulation and connections to optimization; smooth games and saddle points; ML applications and assumptions.</span>
      </td>
      <td class="links">
        <a href="slides/L17-L19.pdf">Slides</a>
        <a href="notes/L17-L19.pdf">Notes</a>
      </td>
    </tr>

    <tr>
      <td>L20–L21</td>
      <td>
        Min-Max Algorithms
        <span class="tag">Part II</span><br>
        <span style="color:#555;">Gradient descent–ascent; extragradient methods; Hamiltonian gradient and consensus optimization (and stochastic variants).</span>
      </td>
      <td class="links">
        <a href="slides/L20-L21.pdf">Slides</a>
        <a href="notes/L20-L21.pdf">Notes</a>
      </td>
    </tr>

    <tr>
      <td>L22–L23</td>
      <td>
        Distributed / Decentralized / Federated Optimization
        <span class="tag">Part III</span><br>
        <span style="color:#555;">Core algorithms; communication-efficient rules (compression, local updates, topology).</span>
      </td>
      <td class="links">
        <a href="slides/L22-L23.pdf">Slides</a>
        <a href="notes/L22-L23.pdf">Notes</a>
      </td>
    </tr>

    <!-- ====== Add more rows as needed ====== -->
  </tbody>
</table>




<br>
<br>
</div>

 </body>
</html>
