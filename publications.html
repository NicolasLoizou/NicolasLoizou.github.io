<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Nicolas Loizou</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26204762-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>


  </head>


  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <ul class="menu">
            <li><a href="index.html">About</a></li>
            <li><a href="news.html"> News </a></li>
            <li><a class="active" href="publications.html">Publications</a></li>
            <li><a href="team.html">Team</a></li>
            <li><a href="talks.html">Talks</a></li>
            <li><a href="bio.html">Bio</a></li>
            <li><a href="ProfServ.html">Professional Services</a></li>
            <li><a href="teaching.html">Teaching</a></li>
            <li><a href="openpositions.html">Open Positions</a></li>
          </ul>
  <br>
          <script src="table_header.js"></script> </div>

      <div id="wrapper" class="main">
        <div id="content">
<br>
<br>
<br>
<br>


<h1>Publications</h1>
<hr>

All papers are listed below by year of submission before they are published, or year of publication.

</ul>
</ul>
<h2>2025</h2>
<ul>
  <li>
      <p><a
          href="https://arxiv.org/abs/2512.02342"> Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients</a>, <br>
      Dimitris Oikonomou, Nicolas Loizou <br>
  </li>
  <li>
      <p><a
          href="https://arxiv.org/abs/2504.01898"> Analysis of an Idealized Stochastic Polyak Method and its Application to Black-Box Model Distillation</a>, <br>
      Robert M. Gower, Guillaume Garrigos, Nicolas Loizou, Dimitris Oikonomou, Konstantin Mishchenko, Fabian Schaipp <br>
  </li>
  <li>
      <p><a
          href="https://arxiv.org/abs/2510.22421"> Extragradient Method for (L_0, L_1)-Lipschitz Root-finding Problems,</a> <br>
      Sayantan Choudhury, Nicolas Loizou <br>
      Thirty-Nine Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2025 <br></p>
  </li>
  <li>
      <p><a
          href="https://arxiv.org/abs/2501.08263"> Multiplayer Federated Learning: Reaching Equilibrium with Less Communication</a>, <br>
      TaeHo Yoon, Sayantan Choudhury, Nicolas Loizou <br>
      Thirty-Nine Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2025 <br></p>
  </li>
  <li>
      <p><a
          href="https://arxiv.org/abs/2503.02225"> Sharpness Aware Minimization: General Analysis and Improved Rates</a>, <br>
      Dimitris Oikonomou, Nicolas Loizou <br> 13th International Conference on Learning Representations (<b>ICLR</b>), 2025 <br>
  </li>
  <li>
      <p><a
          href="https://arxiv.org/abs/2406.04142"> Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance</a>, <br>
      Dimitris Oikonomou, Nicolas Loizou <br> 13th International Conference on Learning Representations (<b>ICLR</b>), 2025 <br>
  </li>
  <li>
      <p><a
          href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jmri.29819"> Federated Learning for Renal Tumor Segmentation and Classification on Multi‐Center MRI Dataset</a>, <br>
      Dat‐Thanh Nguyen, Maliha Imami, Lin‐Mei Zhao, Jing Wu, Ali Borhani, Alireza Mohseni, Mihir Khunte, Zhusi Zhong, Victoria Shi, Sophie Yao, Yuli Wang, Nicolas Loizou, Alvin C Silva, Paul J Zhang, Zishu Zhang, Zhicheng Jiao, Ihab Kamel, Wei‐Hua Liao, Harrison Bai <br> <b>Journal of Magnetic Resonance Imaging</b>, 2025 <br>
  </li>
</ul>
<h2>2024</h2>
<ul>
  <li>
      <p><a
          href="https://nips.cc/virtual/2024/poster/97755">
Enhancing Vision-language Models for Medical Imaging: Bridging the 3D Gap with Innovative Slice Selection</a>, <br>
      Yuli Wang, Peng jian, Yuwei Dai, Craig Jones, Haris I. Sair, Jinglai Shen, Nicolas Loizou, Jing Wu, Wen-Chi Hsu, Maliha Rubaiyat Imami, Zhicheng Jiao, Paul J Zhang, Harrison Bai <br>
      Thirty-Eighth Conference on Neural Information Processing Systems (<b>NeurIPS</b>), D&B Track, 2024 <br></p>
  </li>
  <li>
      <p><a
          href="https://arxiv.org/abs/2403.02648"> Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad</a>, <br>
      Sayantan Choudhury, Nazarii Tupitsa, Nicolas Loizou, Samuel Horvath, Martin Takac, Eduard Gorbunov <br>
      Thirty-Eighth Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2024 <br></p>
  </li>
  <li>
    <p><a
        href="https://arxiv.org/abs/2306.05100"> Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates</a>, <br>
      Siqi Zhang, Sayantan Choudhury, Sebastian U Stich, Nicolas Loizou <br> 12th International Conference on Learning Representations (<b>ICLR</b>), 2024 <br></p>
  </li>
  <li>
      <p><a
          href="https://arxiv.org/abs/2403.07148"> Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities</a>, <br>
      Konstantinos Emmanouilidis, Rene Vidal, Nicolas Loizou <br>
      27th International Conference on Artificial Intelligence and Statistics (<b>AISTATS</b>), 2024 <br></p>
  </li>
  <li>
      <p><a
          href="https://arxiv.org/abs/2403.09090"> Dissipative Gradient Descent Ascent Method: A Control Theory Inspired Algorithm for Min-max Optimization</a>, <br>
      Tianqi Zheng, Nicolas Loizou, Pengcheng You, Enrique Mallada <br>
      <b> IEEE Control Systems Letters </b> <a href="https://ieeexplore.ieee.org/document/10554660"> [Journal] </a>, 2024</p>
  </li>
  <li>
    <p><a
        href="https://arxiv.org/abs/2307.06306"> Locally Adaptive Federated Learning</a>, <br>
      Sohom Mukherjee,  Nicolas Loizou, Sebastian U Stich <br>
      <b>Transactions on Machine Learning Research</b>, 2024  <br></p>
  </li>

</ul>
<h2>2023</h2>
<ul>
  <li>
    <p><a
        href="https://arxiv.org/abs/2302.14043"> Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions</a>, <br>
      Sayantan Choudhury, Eduard Gorbunov, Nicolas Loizou <br>
      Thirty-seventh Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2023 <br></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2206.05825"> A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games</a>, <br>
      Samuel Sokota, Ryan D'Orazio, J Zico Kolter, Nicolas Loizou, Marc Lanctot, Ioannis Mitliagkas, Noam Brown, Christian Kroer<br>
      11th International Conference on Learning Representations (<b>ICLR</b>), 2023 <br>
      short version: Deep Reinforcement Learning Workshop, NeurIPS 2022 </p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2202.07262"> Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods</a>, <br>
      Aleksandr Beznosikov, Eduard Gorbunov, Hugo Berard, Nicolas Loizou<br>
      26th International Conference on
Artificial Intelligence and Statistics (<b>AISTATS</b>), 2023 <br>
      short version: Optimization for Machine Learning Workshop, NeurIPS 2022 </p>
    </li>
    <li>
      <p><a href="https://arxiv.org/abs/2110.15412"> Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize</a>, <br>
        Ryan D'Orazio, Nicolas Loizou, Issam Laradji, Ioannis Mitliagkas <br>
        <b>Transactions on Machine Learning Research</b>, 2023 <a href="https://openreview.net/forum?id=28bQiPWxHl"> [Journal] </a> <br>
        short version: Beyond First-order Methods in ML Systems Workshop, ICML 2021. </p>
    </li>
    <li>
      <p><a href="https://arxiv.org/abs/2006.11573"> Unified Analysis of Stochastic Gradient Methods for
        Composite Convex and Smooth Optimization</a>, <br> Ahmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert M. Gower, Peter Richtárik, <br>
       <b> Journal of Optimization Theory and Applications </b> (2023): 1-42 <a href="https://link.springer.com/article/10.1007/s10957-023-02297-y"> [Journal] </a></p>
        </li>
    <li>
      <p><a href="https://arxiv.org/abs/2102.09700"> AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods</a>, <br> Zheng Shi, Abdurakhmon Sadiev, Nicolas Loizou, Peter Richtárik, Martin Takáč, <br>
      <b>Transactions on Machine Learning Research</b>, 2835-8856, 2023  <a href="https://openreview.net/forum?id=WoXJFsJ6Zw"> [Journal] </a></p>
    </li>
    </ul>
    </ul>
    <h2>2022</h2>
    <ul>
    <li>
    <p><a href="https://openreview.net/forum?id=ct_s9E1saB1"> ProxSkip for Stochastic Variational Inequalities: A Federated Learning Algorithm for Provable Communication Acceleration</a>, <br>
      Siqi Zhang, Nicolas Loizou.<br>
      Optimization for Machine Learning Workshop, NeurIPS 2022 </p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2205.04583"> Dynamics of SGD with Stochastic Polyak Stepsizes: Truly Adaptive Variants and Convergence to Exact Solution</a>, <br>
      Antonio Orvieto, Simon Lacoste-Julien, Nicolas Loizou<br>
      Thirty-sixth Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2022 <br>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.08611"> Stochastic Extragradient: General Analysis and Improved Rates</a>, <br>
      Eduard Gorbunov, Hugo Berard, Gauthier Gidel, Nicolas Loizou<br>
      25th International Conference on
Artificial Intelligence and Statistics (<b>AISTATS</b>), 2022 <br>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.04261"> Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity</a>, <br>
      Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel<br>
      25th International Conference on
Artificial Intelligence and Statistics (<b>AISTATS</b>), 2022 <br>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.00464"> On the Convergence of Stochastic Extragradient for Bilinear Games with Restarted Iteration Averaging</a>, <br>
      Chris Junchi Li, Yaodong Yu, Nicolas Loizou, Gauthier Gidel, Yi Ma, Nicolas Le Roux, Michael I Jordan <br>
      25th International Conference on
Artificial Intelligence and Statistics (<b>AISTATS</b>), 2022 <br>
      short version: Optimization for Machine Learning Workshop, NeurIPS 2021 (<b>Oral Talk</b>)</p>
  </li>
</ul>
</ul>
<h2>2021</h2>
<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2107.00052"> Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity</a>, <br>
      Nicolas Loizou, Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas, Simon Lacoste-Julien<br>
      Thirty-fifth Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2021 <br>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.10311"> SGD for
        Structured Nonconvex Functions: Learning Rates, Minibatching
        and Interpolation</a>, <br> Robert M. Gower,
      Othmane Sebbouh, Nicolas Loizou <br>
      24th International Conference on
Artificial Intelligence and Statistics (<b>AISTATS</b>), 2021 <br>
    short version: Optimization for Machine Learning Workshop, NeurIPS 2020</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.10542"> Stochastic Polyak
        Step-size for SGD: An Adaptive Learning Rate for Fast
        Convergence</a>, <br> Nicolas Loizou, Sharan Vaswani, Issam Laradji and
      Simon Lacoste-Julien.<br>
      24th International Conference on
Artificial Intelligence and Statistics (<b>AISTATS</b>), 2021 <br>
    short version: Optimization for Machine Learning Workshop, NeurIPS 2020 (<b>Spotlight Talk</b>)</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.08645"> Revisiting
        Randomized Gossip Algorithms: General Framework, Convergence
        Rates and Novel Block and Accelerated Protocols</a>,<br> Nicolas Loizou,
      Peter Richtarik <br> <b>IEEE Transactions on Information Theory </b> 67.12 (2021): 8300-8324. <a href="https://ieeexplore.ieee.org/abstract/document/9539193"> [Journal] </a></p>
  </li>
</ul>
</ul>
<h2>2020</h2>
<ul>

  <li>
    <p><a href="https://arxiv.org/abs/2007.04202"> Stochastic Hamiltonian Gradient Methods for Smooth Games</a>,<br>
      Nicolas Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent,
      Simon Lacoste-Julien and Ioannis Mitliagkas. <br>
      Proceedings of the 37th International Conference on Machine
      Learning (<b>ICML</b>), 2020.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.10422"> A Unified Theory
        of Decentralized SGD with Changing Topology and Local
        Updates</a>,<br> Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri,
      Martin Jaggi and Sebastian U. Stich.<br>
      Proceedings of the 37th International Conference on Machine
      Learning (<b>ICML</b>), 2020.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1903.07971"> Convergence
        Analysis of Inexact Randomized Iterative Methods</a>,<br> Nicolas Loizou,
      Peter Richtarik <br>SIAM Journal on Scientific Computing (<b>SISC</b>) 42.6 (2020): A3979-A4016. <a href="https://epubs.siam.org/doi/abs/10.1137/19M125248X"> [Journal] </a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1712.09677"> Momentum and
        Stochastic Momentum for Stochastic Gradient, Newton,
        Proximal Point and Subspace Descent Methods </a>,<br> Nicolas Loizou,
      Peter Richtarik.  <br> Computational Optimization and Applications (<b>COAP</b>) 77.3 (2020): 653-710. <a href="https://link.springer.com/article/10.1007/s10589-020-00220-z"> [Journal] </a>
      <br>  <span class="important">2020 COAP Best Paper Award</span> <a href="https://link.springer.com/article/10.1007/s10589-021-00327-x"> [Link] </a> <br>
       </p>
  </li>
</ul>
<h2>2019</h2>
<ul>
  <li>
    <p> <a href="https://arxiv.org/abs/1909.12176">Randomized Iterative
        Methods for Linear Systems: Momentum, Inexactness and Gossip</a><br>Nicolas Loizou <br>
      PhD Dissertation, The University of Edinburgh, 2019.
      <br>  <span class="important">OR Society’s Doctoral Award (runner-up)</span> <br>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1901.09401"> SGD: General
        Analysis and Improved Rates</a>,<br> Robert Mansel Gower, Nicolas Loizou,
      Xun Qian, Alibek Sailanbayev, Egor Shulgin and Peter
      Richtarik. <br>
      Proceedings of the 36th International Conference on Machine
      Learning (<b>ICML</b>), pages 5200-5209, 2019. </p>
  </li>
  <li>
    <p> <a href="https://arxiv.org/abs/1811.10792"> Stochastic
        Gradient Push for Distributed Deep Learning</a>,<br>
      Mahmoud Assran, Nicolas Loizou, Nicolas Ballas and Michael Rabbat <br>
      Proceedings of the 36th International Conference on Machine
      Learning (<b>ICML</b>), pages 344-353, 2019. <br>
      short version: Systems for ML workshop, NeurIPS 2018. </p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1810.13084"> Provably
        Accelerated Randomized Gossip Algorithms </a>,<br> Nicolas Loizou, Michael
      Rabbat and Peter Richtarik. <br>
      IEEE International Conference on Acoustics, Speech and Signal
      Processing (<b>ICASSP</b>), 2019 </p>
  </li>
</ul>
<h2>2018</h2>
<ul>
  <li>
    <p> <a href="PrivacyGossip.pdf"> A Privacy Preserving
        Randomized Gossip Algorithm via Controlled Noise Insertion</a>,<br>
      with Filip Hanzely, Jakub Konecny, Nicolas Loizou, Peter Richtarik, Dmitry
      Grishchenko. <br>
      Privacy Preserving Machine Learning Workshop-<b>NeurIPS</b>, 2018 </p>
  </li>
  <li>
    <p> <a href="acc_gossip.pdf"> Accelerated Gossip via Stochastic
        Heavy Ball Method </a>,<br> Nicolas Loizou, Peter Richtarik. <br>
      56th Annual Allerton Conference on Communication, Control, and
      Computing (<b>Allerton</b>), 2018 <a href="PosterSHBConc.pdf">[poster]</a> </p>
  </li>
</ul>
<h2>2017</h2>
<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1710.10737"> Linearly
        convergent stochastic heavy ball method for minimizing
        generalization error </a>,<br> Nicolas Loizou, Peter Richtarik. <br>
      NIPS Workshop on Optimization for Machine Learning 2017 <a
        href="NIPS_PosterSHB.pdf">[poster]</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1706.07636"> Privacy
        Preserving Randomized Gossip Algorithms</a>,<br>  Filip
      Hanzely, Jakub Konecny, Nicolas Loizou, Peter Richtarik, Dmitry Grishchenko</p>
  </li>
</ul>
<h2>2016 or earlier </h2>
<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1610.04714"> A New Perspective
        on Randomized Gossip Algorithms </a>,<br>Nicolas Loizou, Peter Richtarik.<br>
      IEEE Global Conference on Signal and Information Processing
      (<b>GlobalSIP</b>), pp.440-444, 2016 <a href="PosterGlobalSip.pdf">[poster]</a>
    </p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1610.00651"> Distributionally
        Robust Games with Risk-averse Players</a>,<br>Nicolas Loizou <br>
      Proceedings of the 5th International Conference on Operations
      Research and Enterprise Systems (<b>ICORES</b>), ISBN 978-989-758-171-7, pages
      186-196, 2016 </p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1512.03253"> Distributionally
        Robust Game Theory </a><br>Nicolas Loizou <br> MSc Thesis, Imperial College
      London, 2015.</p>
  </li>

</ul>

<br>
<br>
</div>

 </body>
</html>
