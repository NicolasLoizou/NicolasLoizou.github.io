<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Nicolas Loizou</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26204762-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>


  </head>


  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <ul class="menu">
            <li><a href="index.html">About</a></li>
            <li><a class="active" href="news.html"> News </a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="team.html">Team</a></li>
            <li><a href="talks.html">Talks</a></li>
            <li><a href="bio.html">Bio</a></li>
            <li><a href="ProfServ.html">Professional Services</a></li>
            <li><a href="teaching.html">Teaching</a></li>
            <li><a href="openpositions.html">Open Positions</a></li>
          </ul>
  <br>
          <script src="table_header.js"></script> </div>

      <div id="wrapper" class="main">
        <div id="content">
<br>
<br>
<br>
<br>

<h1>News</h1>
<hr>
<br>
<ul>
  <p>
    <li>
      <p>  <strong>08 Dec 2025:</strong>

        After NeurIPS 2025, I’m staying in California for a few more days. Today, I’m giving a talk at the
  <a href="https://secure.math.ucla.edu/seminars/display.php?&id=839150">Applied Math Colloquium</a>
  in the <a href="https://ww3.math.ucla.edu/">UCLA Department of Mathematics</a>. I’ll present some of our
  recent work at the intersection of game theory and federated learning.
     Talk title: <b>Communication in Multiplayer Games Matters: A Federated Learning Approach to Equilibrium Computation.</b>
     <br>
    </p>
    <li>
      <p>  <strong>23 Nov 2025:</strong>
    I have recently joined the editorial boards of:<br>
     <em> <a href="https://www.tandfonline.com/journals/goms20"> Optimization Methods and Software</a></em>, and <em> <a href="https://jmlr.org/tmlr/index.html"> Transactions on Machine Learning Research. </a></em><br>
    Please consider submitting your best work at the intersection of optimization and machine learning!
  </p>

    <li>
      <p>
        <strong><time datetime="2025-10-07">07 Oct 2025</time>:</strong>
        Recent highlights
        <em>(posted after I enjoyed time with our newborn)</em>:
      </p>
      <ul>
        <li>
          <p>
            <strong><time datetime="2025-09">September 2025</time></strong> &mdash;
            I’m delighted to share that my first doctoral student, <a href="https://isayantan.github.io/"> Sayantan Choudhury</a>, successfully defended his PhD dissertation.
            We began working together in Summer 2022, and over the past three years it has been a joy to watch him grow into a very strong researcher. During his PhD he co-authored seven papers (including 4 NeurIPS and 2 ICLR).
            His thesis entitled  <b>"Next-Generation Iterative Algorithms for Large-Scale Min-Max Optimization: Design and Analysis"</b> will be available online soon.
          </p>
        </li>

        <li>
          <p>
            <strong><time datetime="2025-09">September 2025</time></strong> &mdash;
            Two papers led by members of my group were accepted to <b>NeurIPS 2025</b>:
          </p>
          <ul>
            <li><em><a href="https://arxiv.org/abs/2501.08263"> Multiplayer Federated Learning: Reaching Equilibrium with Less Communication</a></em>, <br> TaeHo Yoon, Sayantan Choudhury, Nicolas Loizou.</li>
            <li><em> <a href="https://neurips.cc/virtual/2025/poster/116461"> Extragradient Method for (L_0, L_1)-Lipschitz Root-finding Problems</a></em>, <br> Sayantan Choudhury, Nicolas Loizou.</li>
          </ul>
        </li>

        <li>
          <p>
            <strong><time datetime="2025-08">August 2025</time></strong> &mdash;
            Together with <a href="https://ernestryu.com/"> Ernest Ryu (UCLA)</a>, we received an NSF CISE Medium award for the project
            &ldquo;<a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2504626&HistoricalAwards=false">Post-Modern Min-Max Optimization Theory: Departure from Classical Minimization Theory</a>.&rdquo;
            The project focuses on developing specialized theoretical frameworks and efficient algorithms
            tailored to unconstrained min-max optimization.
          </p>
        </li>

        <li>
          <p>
            <strong><time datetime="2025-06">June 2025</time></strong> &mdash;
            I joined the editorial board of <em> <a href="https://academic.oup.com/imaiai/pages/Editorial_Board"> Information and Inference: A Journal of the IMA</a></em>.
            Please consider submitting your best work in optimization and machine learning!
          </p>
        </li>

        <li>
          <p>
            <strong><time datetime="2025-05">May 2025</time></strong> &mdash;
            Our paper <em><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jmri.29819"> Federated Learning for Renal Tumor Segmentation and Classification on Multi-Center MRI Dataset</a></em>
            was accepted for publication in the <em><b>Journal of Magnetic Resonance Imaging</b></em>.
          </p>
        </li>

        <li>
          <p>
            <strong><time datetime="2025-01">January 2025</time></strong> &mdash;
            Two papers, joint work with <a href="https://dimitris-oik.github.io/"> Dimitris Oikonomou</a>, were accepted to ICLR 2025:
          </p>
          <ul>
            <li><em><a href="https://arxiv.org/abs/2503.02225"> Sharpness Aware Minimization: General Analysis and Improved Rates</a></em>.</li>
            <li><em><a href="https://arxiv.org/abs/2406.04142"> Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance</a></em>.</li>
          </ul>
        </li>
    </li>
    </ul>
    <li>
    <p> 22 Jan 2025: Two papers, co-authored with <a href="https://dimitris-oik.github.io/"> Dimitris Oikonomou</a>,  were accepted to <b>ICLR 2025</b>:</p>
      <ul>
  <li><a
    href="https://openreview.net/forum?id=8rvqpiTTFv">Sharpness Aware Minimization: General Analysis and Improved Rates</a> </li>
  <li><a
      href="https://arxiv.org/abs/2406.04142">Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance</a> </li>
  </ul>
      <li>
        <p> 15 Jan 2025: I will be attending the <a
            href="https://energyinstitute.jhu.edu/pec-events/rosei-summit-2025/">
           ROSEI Summit 2025</a> at JHU. During the afternoon session, I will present key highlights of our ongoing work on large-scale min-max optimization and its connections to energy systems.
      </li>
          <li>
        <p> 27 Oct 2024: Over the next 4 weeks, I will be giving a few presentations on our recent and ongoing work in stochastic min-max optimization, variational inequalities (VIPs), and adaptive methods. Slides and videos will be available after the presentations. I look forward to meeting with colleagues at Wisconsin-Madison, Minnesota, Princeton, and Rice. </p>
          <ul>
      <li>25 Nov 2024: <a
          href="https://cmor.rice.edu/news-events/colloquia">
         Rice CMOR Department  Colloquium</a>,  Computational Applied Mathematics & Operations Research Department, Rice University. </li>
      <li>21 Nov 2024: <a
          href="https://orfe.princeton.edu/events">
         Princeton Optimization Seminar </a>,  Operations Research & Financial Engineering Department, Princeton University. </li>
         <li>12 Nov 2024: <a
             href="https://cse.umn.edu/dsi/cse-dsi-machine-learning-seminar-series">
            CSE DSI Machine Learning Seminar Series </a>,  University of Minnesota. </li>
         <li>30 Oct 2024: <a
             href="https://silo.wisc.edu/">
            SILO Seminar</a>,  University of Wisconsin-Madison. </li>
      </ul>
      <li>
      <p> 25 Sept 2024: Two papers were accepted to <b>NeurIPS 2024</b>:</p>
        <ul>
    <li><a
      href="https://arxiv.org/abs/2403.02648"> Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad</a> </li>
    <li><a
        href="https://nips.cc/virtual/2024/poster/97755">
Enhancing Vision-language Models for Medical Imaging: Bridging the 3D Gap with Innovative Slice Selection</a> </li>
    </ul>
    <li>
      <p> 22 July 2024: On my way to <a href="https://ismp2024.gerad.ca/"> 25th International Symposium on Mathematical Programming (ISMP 2024)</a>. Tomorrow, I’ll be presenting our latest findings on Stochastic Extragradient Methods at the <a href="https://ismp2024.gerad.ca/schedule/TB/67"> Numerical Optimization for Machine Learning</a> session.
On Thursday, we organize a minisymposium on
<a href="https://ismp2024.gerad.ca/schedule/ThA/128"> Adaptive, Line-search, and Parameter-Free Stochastic Optimization</a>.
    </li>

    <li>
      <p> 18 June 2024: During summer, I am based in Toronto at <a href="https://vectorinstitute.ai/"> Vector Institute</a>. Today I give a talk at the institute's weekly seminar on "Scalable Optimization Algorithms for Large-Scale Machine Learning Models".
    </li>
    <li>
      <p> 06 May 2024: This week, I am visting the <a href="https://www.imperial.ac.uk/computing/"> Department of Computing</a> at Imperial College London. On Tuesday, I will give a talk at the Computational Optimisation Seminar on "Next-Generation Adaptive Optimization Algorithms for Large-Scale Machine Learning Models".

        <p> I had a great time catching up with <a href="https://www.doc.ic.ac.uk/~pp500/"> Panos Parpas</a>, <a href="https://wp.doc.ic.ac.uk/wwiesema/"> Wolfram Wiesemann</a>, and <a href="https://wp.doc.ic.ac.uk/rmisener/"> Ruth Misener</a>!
    </li>
    <li>
      <p> 02-04 May 2024: I am attending <a
          href="https://aistats.org/aistats2024/">
         AISTATS 2024 </a> in Valencia, Spain. <p>  <a href="https://emmanouilidisk.github.io/"> Konstantinos </a> is also here, presenting our work: <a href="https://proceedings.mlr.press/v238/emmanouilidis24a.html"> Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities</a>.
    </li>
    <li>
      <p> 22-24 Mar 2024: I am attending <a
          href="https://ios2024.rice.edu/">
         2024 INFORMS Optimization Society Conference (IOS 2023)</a> at Houston, Texas. On Sunday, 24 March 2024, I will be presenting some of our latest findings on adaptive optimization algorithms. Title of the talk:  "Advancing SGD: Exploring the Interplay of Stochastic Polyak Step-size and Momentum." </p>
    </li>
    <li>
      <p> 19 Jan 2024: Our paper "Stochastic Extragradient with Random Reshuffling:
Improved Convergence for Variational Inequalities", joint work with Konstantinos Emmanouilidis and Rene Vidal,
        was accepted to <a href="https://aistats.org/aistats2024/"> AISTATS 2024. </a> </p>
        <p> Special congrats to <a href="https://emmanouilidisk.github.io/"> Konstantinos Emmanouilidis </a> on the acceptance of his first paper!
    </li>
    <li>
    <p> 16 Jan 2024: Our paper <a href="https://arxiv.org/abs/2306.05100"> Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates</a>, joint work with Siqi Zhang, Sayantan Choudhury, and Sebastian U Stich
      was accepted to <a href="https://iclr.cc/Conferences/2024"> ICLR 2024. </a> </p></a>
  </li>
    <li>
      <p> 10 Dec 2023: I am traveling to New Orleans to attend <a href="https://neurips.cc/"> NeurIPS 2023</a>. <p>  <a href="https://isayantan.github.io/"> Sayantan Choudhury</a> is also here, presenting our work: <a href="https://arxiv.org/abs/2302.14043"> Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions</a>.
    </li>
    <li>
      <p> 15 Nov 2023: This week, I am co-organizing the <a href="https://deepmath-conference.com/"> DeepMath 2023</a> Conference on the Mathematical Theory of Deep Neural Networks, which takes place at JHU. I am looking forward to the excellent set of speakers and the poster sessions.
    </li>
    <li>
      <p> 18 Oct 2023: This week, I am visting the <a href="https://uwaterloo.ca/combinatorics-and-optimization/"> Combinatorics and Optimization Department</a> at the University of Waterloo. On Friday, 20 October, I will give a talk at the Continous Optimization Seminar on "Next-Generation Adaptive Optimization Algorithms for Large-Scale Machine Learning Models".

        <p> <a href="https://uwaterloo.ca/scholar/vavasis"> Steve Vavasis</a> and <a href="https://cs.uwaterloo.ca/~y328yu/"> Yaoliang Yu</a> thanks for the great hospitality!
    <li>
      <p> 22 Sept 2023: Our paper <a href="https://arxiv.org/abs/2302.14043"> Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions</a>, <br>
        joint work with Sayantan Choudhury, and Eduard Gorbunov
        was accepted to <a href="https://neurips.cc/"> NeurIPS 2023. </a> </p>

        <p> Special congrats to <a href="https://sites.google.com/view/sayantan-homepage/home"> Sayantan Choudhury</a> on the acceptance of his first paper!
    </li>

    <li>
      <p> 18 Sept 2023: I am visiting <a href="https://www.simonsfoundation.org/flatiron/"> Flatiron Institute </a> in New York City this week.  </p>
    </li>

  <li> 28 Aug. 2023: <p> <b>New Phd Student: Dimitris Oikonomou </b>

  <p>   <a href="https://dimoik96.github.io/"> Dimitris Oikonomou </a>  is a new member in my Lab. He has just started his Ph.D. in the CS department.

    <p> Dimitris got his BSc in Mathematics from the National and Kapodistrian University of Athens. He then obtained two MSE degrees: MSc in Mathematics from the University of Gottingen, Germany and
  MSc in Data Science and Machine Learning from National Technical University of Athens. All BSc and MSc degrees with distinction!

  <p> He was also very active in Mathematical Olympiads competitions:

  <ul>
<li>International Mathematical Olympiad (IMO): Bronze Medal  (Colombia 2013) </li>
<li>Greek Mathematical Olympiad: Silver Medal (2014)</li>
<li>  Greek Mathematical Olympiad: Bronze Medal (2013) </li>
</ul>

  <p> Dimitri, welcome to the team!
</li>

<li>
  <p> 21 Aug 2023: Our paper <a href="https://arxiv.org/abs/2006.11573"> Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization
</a>, joint work with Ahmed Khaled, Othmane Sebbouh, Robert M. Gower, and Peter Richtárik
    was accepted to <b> Journal of Optimization Theory and Applications (JOTA)</b></p>
</li>

<li>
  <p> 12 Jul 2023: <b>New Paper out:</b> <a
      href="https://arxiv.org/abs/2307.06306"> Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes,
</a> joint work with Sohom Mukherjee, and Sebastian U Stich. </p>
</li>
    <li>
      <p> 08 Jun 2023: <b>New Paper out:</b> <a
          href="https://arxiv.org/abs/2306.05100"> Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates
    </a>, joint work with Siqi Zhang, Sayantan Choudhury, and Sebastian U Stich. </p>
    </li>
    <li>
      <p> 31 May 2023:  Attending <a
          href="https://www.siam.org/conferences/cm/conference/op23">  SIAM Conference on Optimization (OP23)
</a> in Seattle this week!
Reach out if you would like to meet.

On June 1, with <a href="https://siqi-z.github.io/"> Siqi Zhang</a> and
<a href="https://sites.google.com/view/sayantan-homepage/home"> Sayantan Choudhury </a>, we organize a 4-session mini-symposium on <a href="https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=76806"> "Recent Advancements in Optimization Methods for Machine Learning". </a>
</p>
<li>
  <p> 22-24 Mar 2023: I am attending <a
      href="https://ciss.jhu.edu/">
     57th Annual Conference on Information Science and Systems (CISS 2023)</a> at JHU. I am organizing a session on Optimization Methods for Machine Learning and giving a talk in the Estimation & Learning in Stochastic Systems session organized by James Spall.</p>
</li>
    <li>
      <p> 03 March 2023: <b>CISCO Research Gift: </b> Thrilled to receive a <a
          href="https://research.cisco.com/">  CISCO Research
</a> monetary gift for working on the foundations of Multi-player Federated Learning. </p>
    </li>
    <li>
      <p> 27 Feb 2023: <b>New Paper out:</b> <a
          href="https://arxiv.org/abs/2302.14043"> Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions
    </a>, joint work with Sayantan Choudhury and Eduard Gorbunov. </p>
    </li>
    <li>
      <p> 22-24 Feb 2023: I am attending <a href="https://studentconference.csl.illinois.edu/"> CSLSC 2023
    </a> at the University of Illinois at Urbana-Champaign this week. I will give the keynote presentation at the <a href="https://studentconference.csl.illinois.edu/optimization-control-and-reinforcement-learning/"> Optimization, Control and Reinforcement Learning Session
    </a> on Friday, 24 Feb. Please reach out if you want to meet.
    </p>
    </li>

    <li>
      <p> 20 Feb 2023: Our paper <a href="https://arxiv.org/abs/2102.09700"> AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods
    </a>, joint work with Zheng Shi, Abdurakhmon Sadiev, Peter Richtárik, and Martin Takáč
        was accepted to <b>Transactions on Machine Learning Research (TMLR)</b></p>
    </li>
    <li>
      <p> 20 Jan 2023:  <a href="http://aistats.org/aistats2023/"> AISTATS 2023 </a> and <a href="https://iclr.cc/">ICLR 2023 </a> decisions are out:

      <p>  Our paper <a href="https://arxiv.org/abs/2202.07262"> Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods,
</a> joint work with Aleksandr Beznosikov, Eduard Gorbunov, and Hugo Berard,
        was accepted to <b>AISTATS 2023</b>. </p>

        <p> Our paper <a href="https://arxiv.org/abs/2206.05825"> A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games</a>, joint work with
          Samuel Sokota, Ryan D'Orazio, J Zico Kolter, Marc Lanctot, Ioannis Mitliagkas, Noam Brown, and Christian Kroer
          was accepted to <b>ICLR 2023</b>.
    </li>
    <li>
      <p> 14 Dec 2022: Since 05 Dec 2022, I have been a research visitor at <a href="https://vectorinstitute.ai/"> Vector Institute
</a> in Toronto.  If you are a student or researcher in Toronto, please feel free to drop me a message. I would be happy to arrange a meeting. I will be here until 18 Jan 2023. </p>
    </li>
    <li>
      <p> 28 Nov 2022: I am attending <a href="https://neurips.cc/"> NeurIPS 2022 </a> this week. Please drop me a message if you would like to meet.

      <p>  We are presenting our work <a href="https://openreview.net/forum?id=lUyAaz-iA4u"> Dynamics of SGD with Stochastic Polyak Stepsizes: Truly Adaptive Variants and Convergence to Exact Solution
</a> in the main conference. Please join us during NeurIPS' <a href="https://neurips.cc/virtual/2022/poster/53451"> first poster session</a> tomorrow (29 Nov).

<p> Three more papers have been accepted at NeurIPS workshops.
    </li>
    <li>
      <p> 14 Sept 2022: Our paper <a href="https://arxiv.org/abs/2205.04583"> Dynamics of SGD with Stochastic Polyak Stepsizes: Truly Adaptive Variants and Convergence to Exact Solution
</a>, joint work with Antonio Orvieto and Simon Lacoste-Julien,
        was accepted to <b>NeurIPS 2022</b>. </p>
    </li>
    <li>
      <p> 29 Aug 2022: The Fall 2022 semester begins. I am teaching the new graduate course, "EN.553.669: Large Scale Optimization for Data Science," offered for the first time at JHU this year.
    </li>
    <li>
      <p> 26 Jul 2022: I am attending <a href="https://iccopt2022.lehigh.edu/"> The seventh International Conference on Continuous Optimization (ICCOPT)</a> this week.
         Today I am presenting our recent work <a href="https://arxiv.org/abs/2202.07262"> Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods</a>. <br> ICCOPT 2022 is having <a href="https://iccopt2022.lehigh.edu/scientific-program/cluster-information/">twelve clusters</a> this year.  <a href="https://www.sstich.ch/">Sebastian Stich </a> and I are the cluster chairs of the Optimization for Data Science and Machine Learning cluster. You are all welcome to our sessions!
    </li>
    <li>
      <p> 17 Jul 2022: I am attending <a href="https://icml.cc/"> ICML 2022.</a> this week, which conveniently is taking place in Baltimore this year. Please drop me a message if you are around, and we can catch up.
    </li>
    <li>
      <p> 22 Apr 2022: I have been selected as a “Highlighted Reviewer of ICLR 2022”. All reviewers are listed on the <a href="https://iclr.cc/Conferences/2022/Reviewers"> ICLR website.</a>
    </li>
    <li>
      <p> 08 Mar 2022: I am giving a talk at MINDS/CIS Seminar Series at JHU. My talk is entitled “Stochastic Iterative Methods for Smooth Games: Practical Variants and Convergence Guarantees" and a recording is available <a href="https://wse.zoom.us/rec/play/WJltEMfmBDH_mGoPqbi8SDRWR0hd5EcTjQ_RpJo6u_6byYJhZicpUZNvdvmBJw7Mqntng6xRMyFvNH-y.86pmJV8KHsZdkeZl?startTime=1646758778000&_x_zm_rtaid=f2FI62UiSCSGQwylkmZ86w.1663632174227.e84371dc342159a891a526894221acb1&_x_zm_rhtaid=43"> here.</a>
    </li>
    <li>
      <p> 18 Jan 2022: Three papers were accepted to <b>AISTATS 2022</b> (25th International Conference on
Artificial Intelligence and Statistics):</p>
      <p><a href="https://arxiv.org/abs/2111.08611"> Stochastic Extragradient: General Analysis and Improved Rates</a>,<br>
        joint work with Eduard Gorbunov, Hugo Berard, and Gauthier Gidel. </p>
    <p><a href="https://arxiv.org/abs/2110.04261"> Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity</a> <br>
        joint work with Eduard Gorbunov, and Gauthier Gidel.</p>
        <p><a href="https://arxiv.org/abs/2107.00464"> On the Convergence of Stochastic Extragradient for Bilinear Games with Restarted Iteration Averaging</a> <br>
            joint work with Chris Junchi Li, Yaodong Yu, Gauthier Gidel, Yi Ma, Nicolas Le Roux, Michael I Jordan.</p>
    </li>
    <li>
      <p> 07 Jan 2022: I officially started as an Assistant Professor in the <a
                 href="https://engineering.jhu.edu/ams/">
                 Department of Applied
     Mathematics and Statistics</a> (AMS) and the <a
         href="https://www.minds.jhu.edu/about-minds-2/">
         Mathematical Institute for Data Science </a> (MINDS), at <a
                 href="https://www.jhu.edu/">
                 Johns Hopkins University</a>!!!
               </li>
    <li>
      <p> 06 Dec 2021: I am attending <a href="https://neurips.cc/"> NeurIPS 2021, </a> this week. <br>
        I am presenting our work <a href="https://arxiv.org/abs/2107.00052"> Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity</a>.
    <li>
      <p> 25 Oct 2021: I am attending <a href="http://meetings2.informs.org/wordpress/anaheim2021/"> 2021 INFORMS Annual Meeting</a> this week. <br>
        Today I am presenting our recent work <a href="https://arxiv.org/abs/2107.00052"> Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity</a> at the virtual session <a href="https://cattendee.abstractsonline.com/meeting/10390/search?query=%40VirtualSessions%7EVirtual+Sessions&ad=search-results-banner_sessions&currentPage=1&searchId=1311"> Recent Advances in Stochastic Gradient Algorithms</a>.
    </li>
    <li>
      <p> 05 Oct 2021: <b>2020 COAP Best Paper Award</b> <br> Our paper <a href="https://link.springer.com/article/10.1007/s10589-020-00220-z"> Momentum and
          Stochastic Momentum for Stochastic Gradient, Newton,
          Proximal Point and Subspace Descent Methods</a>, published in Computational Optimization and Applications (COAP) was voted by the editorial board as the best paper appearing in the journal in 2020!
    </li>
    <li>
      <p> 28 Sept 2021: Our paper <a href="https://arxiv.org/abs/2107.00052"> Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity</a>, joint work with Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas and Simon Lacoste-Julien,
        was accepted to <b>NeurIPS 2021</b> </p>
    </li>
    <li>
      <p> 29 Apr 2021: Our paper <a href="https://arxiv.org/abs/1905.08645">"Revisiting Randomized Gossip Algorithms: General Framework, Convergence Rates and Novel Block and Accelerated Protocols"</a>, joint work with Peter Richtarik,
        was accepted to
<a href="https://ieeeittrans.ee.technion.ac.il/"> IEEE Transactions on Information Theory. </a> </p>
    </li>
    <li>
      <p> 22 Jan 2021: Two papers were accepted to <b>AISTATS 2021</b> (24th International Conference on
Artificial Intelligence and Statistics):</p>
      <p><a href="https://arxiv.org/abs/2002.10542">Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence </a><br>
        joint work with Sharan Vaswani, Issam Laradji and Simon Lacoste-Julien. </p>
    <p><a href="https://arxiv.org/abs/2006.10311">SGD for structured nonconvex functions: Learning rates, minibatching and interpolation </a> <br>
        joint work with Robert M. Gower and Othmane Sebbouh.</p>
    </li>
    <li>
      <p> 16 Oct 2020: I am delighted to be selected as the runner-up of the <a href="https://www.theorsociety.com/membership/awards-medals-and-scholarships/the-doctoral-award/"> OR Society’s Doctoral Award </a> for 2019.
This is an award for the "Most Distinguished Body of Research leading to the Award of a Doctorate in the field of Operational Research" in the United Kingdom.
</li>
    <li>
      <p> 19 Aug 2020: Our paper <a href="https://arxiv.org/abs/1903.07971">"Convergence Analysis of Inexact Randomized Iterative Methods"</a>, joint work with Peter Richtarik,
        was accepted to
<a href="https://www.siam.org/publications/journals/siam-journal-on-scientific-computing-sisc">SIAM Journal on Scientific Computing</a> (<b>SISC</b>) </p>
    </li>
    <li>
      <p> 19 Aug 2020: Our paper <a href="https://arxiv.org/abs/1712.09677">"Momentum and Stochastic Momentum for Stochastic Gradient, Newton, Proximal Point and Subspace Descent Methods
"</a>, joint work with Peter Richtarik,
        was accepted to
<a href="https://www.springer.com/journal/10589">Computational Optimization and Applications</a> (<b>COAP</b>) </p>
    </li>
    <li>
      <p> 20 Jun 2020: <b>New Paper out:</b> <a
          href="https://arxiv.org/abs/2006.11573"> Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization
</a>, joint work with Ahmed Khaled, Othmane Sebbouh, Robert M. Gower and Peter Richtárik. </p>
    </li>
    <li>
      <p> 18 Jun 2020: <b>New Paper out:</b> <a
          href="https://arxiv.org/abs/2006.10311"> SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation</a>, joint work with Robert M. Gower and Othmane Sebbouh. </p>
    </li>
    <li>
      <p> 01 Jun 2020: Two papers were accepted to <b>ICML 2020</b> (37th
        International Conference on Machine Learning):</p>
      <p><a href="https://arxiv.org/abs/2003.10422">A Unified
          Theory of Decentralized SGD with Changing Topology and
          Local Updates </a><br>
        joint work with Anastasia Koloskova, Sadra Boreiri, Martin
        Jaggi and Sebastian U. Stich. </p>
    <p><a href="https://arxiv.org/abs/2007.04202">Stochastic Hamiltonian Gradient Methods for Smooth
    Games </a> <br>
        joint work with Hugo Berard, Alexia Jolicoeur-Martineau,
        Pascal Vincent, Simon Lacoste-Julien and Ioannis
        Mitliagkas.</p>
    </li>
    <li>
      <p> 23 Mar 2020: <b>New Paper out:</b> <a
          href="https://arxiv.org/abs/2003.10422"> A Unified
          Theory of Decentralized SGD with Changing Topology and
          Local Updates</a>, joint work with Anastasia Koloskova,
        Sadra Boreiri, Martin Jaggi and Sebastian U. Stich. </p>
    </li>
    <li>
      <p> 24 Feb 2020: <b>New Paper out:</b> <a
          href="https://arxiv.org/abs/2002.10542"> Stochastic
          Polyak Step-size for SGD: An Adaptive Learning Rate for
          Fast Convergence</a>, joint work with Sharan Vaswani,
        Issam Laradji and Simon Lacoste-Julien. </p>
    </li>
    <li>
      <p> 19 December 2019: I am delighted to be awarded the <a
          href="https://ivado.ca/en/ivado-scholarships/postdoctoral-scholarships/">
          IVADO Fellow Postdoctoral Scholarship </a>. <br>
        For more details on The Institute for Data Valorisation
        (IVADO) and its mission check out the <a
          href="https://ivado.ca/en/about-us/"> IVADO's website </a>
      </p>
    </li>
    <li>
      <p> 01 September 2019: Officially started as a Postdoctoral
        Fellow at <a href="https://mila.quebec/en/"> Mila -
          Quebec Artificial Intelligence Institute</a>. </p>
    </li>
    <li>
      <p> 28 June 2019: <b>Thesis Defense!</b> <br>
        I have successfully defended my PhD thesis "Randomized
        Iterative Methods for Linear Systems: Momentum,
        Inexactness and Gossip" today. A copy of the final version
        of the manuscript is available <a
          href="PhDThesis_Loizou2019.pdf">here</a> </p>
    </li>
    <li>
      <p> 20 May 2019: <b>New Paper out: </b> <a
          href="RandGossip.pdf"> Revisiting Randomized Gossip
          Algorithms: General Framework, Convergence Rates and
          Novel Block and Accelerated Protocols</a>. joint work
        with Peter Richtarik </p>
    </li>
    <li>
      <p> 22 Apr 2019: Two papers accepted to ICML 2019 (36th
        International Conference on Machine Learning):</p>
      <p><a href="https://arxiv.org/abs/1901.09401">SGD: General
          Analysis and Improved Rates</a>. joint work with Robert
        Mansel Gower, Xun Qian, Alibek Sailanbayev, Egor Shulgin
        and Peter Richtarik. </p>
      <p><a href="https://arxiv.org/abs/1811.10792">Stochastic
          Gradient Push for Distributed Deep Learning</a>. joint
        work with Mahmoud Assran, Nicolas Ballas and Michael
        Rabbat </p>
    </li>
    <li>
      <p> 07 Apr - 14 May 2019: I am visiting <a
          href="http://www.maths.ed.ac.uk/%7Erichtarik/index.html">
          Peter Richtarik</a> at <a
          href="https://vcc.kaust.edu.sa/Pages/Home.aspx"> KAUST </a>
      </p>
    </li>
    <li>
      <p> 19 Mar 2019: <b>New Paper out: </b> <a
          href="https://arxiv.org/abs/1903.07971"> Convergence
          Analysis of Inexact Randomized Iterative Methods</a>.
        joint work with Peter Richtarik. </p>
    </li>
    <li>
      <p> 01 Feb 2019: The paper <a
          href="https://arxiv.org/abs/1810.13084"> Provably
          Accelerated Randomized Gossip Algorithms</a>, coauthored
        with Mike Rabbat and Peter Richtarik, was accepted to the
        44th International Conference on Acoustics, Speech, and
        Signal Processing (ICASSP 2019). </p>
    </li>
    <li>
      <p> 27 Jan 2019: <b>New Paper out: </b> <a
          href="https://arxiv.org/abs/1901.09401"> SGD: General
          Analysis and Improved Rates</a>. joint work with Robert
        Mansel Gower, Xun Qian, Alibek Sailanbayev, Egor Shulgin
        and Peter Richtarik. </p>
    </li>
    <li>
      <p> 27 Nov 2018: <b>New Paper out: </b> <a
          href="https://arxiv.org/abs/1811.10792"> Stochastic
          Gradient Push for Distributed Deep Learning</a>. joint
        work with Mahmoud Assran, Nicolas Ballas and Michael
        Rabbat </p>
    </li>
    <li>
      <p> 31 Oct 2018: <b>New Paper out: </b> <a
          href="https://arxiv.org/abs/1810.13084"> Provably
          Accelerated Randomized Gossip Algorithms</a>. joint work
        with Michael Rabbat and Peter Richtarik </p>
    </li>
    <li>
      <p> 13 August - 17 Aug 2018: I am attending <a
          href="http://coral.ie.lehigh.edu/%7Emopta//">
          DIMACS/TRIPODS/MOPTA </a>, this week. On Tuesday I am
        presenting a poster and on Thursday I am giving a talk on
        ''Revisiting Randomized Gossip Algorithms". </p>
    </li>
    <li>
      <p> 09 Jul 2018: <b>New Paper out: </b> <a
          href="acc_gossip.pdf"> Accelerated Gossip via Stochastic
          Heavy Ball Method </a>. The paper is accepted to 56th
        Annual Allerton Conference on Communication, Control, and
        Computing, 2018 </p>
    </li>
    <li>
      <p> 01 Jul - 06 Jul 2018: I am attending <a
          href="https://ismp2018.sciencesconf.org/"> 23rd
          International Symposium on Mathematical Programming </a>
        this week. On Friday I am presenting our latest work:
        ''Convergence Analysis of Inexact Randomized Iterative
        Methods" </p>
    </li>
    <li>
      <p> 10 Apr - 15 May 2018: I am visiting <a
          href="http://mtakac.com/"> Martin Takac </a> and the <a
          href="http://optml.lehigh.edu/people/"> Optimization and
          Machine Learning Research Group at Lehigh University </a>
      </p>
    </li>
    <li>
      <p> 01 Feb - 21 Mar 2018: I am visiting <a
          href="http://www.maths.ed.ac.uk/%7Erichtarik/index.html">
          Peter Richtarik</a> at <a
          href="https://vcc.kaust.edu.sa/Pages/Home.aspx"> KAUST </a>
      </p>
    </li>
    <li>
      <p> 22 Dec 2017: <b>New Paper out: </b> <a
          href="https://arxiv.org/abs/1712.09677"> Momentum and
          Stochastic Momentum for Stochastic Gradient, Newton,
          Proximal Point and Subspace Descent Methods </a> </p>
    </li>
    <li>
      <p> 04-09 Dec 2017: I am attending <a
          href="https://nips.cc/Conferences/2017"> NIPS</a> this
        week. Our work <a href="https://arxiv.org/abs/1710.10737">
          Linearly convergent stochastic heavy ball method for
          minimizing generalization error </a> is presented at <a
          href="http://opt-ml.org/"> NIPS Workshop on Optimization
          for Machine Learning </a> </p>
    </li>
    <li>
      <p> 1-10 Oct 2017: I am visiting <a
          href="https://simons.berkeley.edu/"> Simons Institute
          for the Theory of Computing </a> at Berkeley,
        California. I am attending the workshop <a
          href="https://simons.berkeley.edu/workshops/optimization2017-2">
          Fast Iterative Methods in Optimization </a>. </p>
    </li>
  </ul>
</ul>

<br>
<br>
</div>

 </body>
</html>
