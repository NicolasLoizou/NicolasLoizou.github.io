<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Nicolas Loizou</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26204762-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>


  </head>


  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <ul class="menu">
            <li><a class="active" href="index.html">About Me</a></li>
            <li><a href="news.html"> News </a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="talks.html">Talks</a></li>
            <li><a href="bio.html">Bio</a></li>
            <li><a href="ProfServ.html">Professional Services</a></li>
            <li><a href="teaching.html">Teaching</a></li>
          </ul>
          <br>
          <script src="table_header.js"></script> </div>

      <div id="wrapper" class="main">
        <div id="content">

<br>
<br>
<br>
<br>
<h1>About Me</h1>
<hr> <br>
I am a postdoctoral fellow at <a href="https://mila.quebec/en/">
  Mila - Quebec Artificial Intelligence Institute</a> hosted by <a
  href="http://www.iro.umontreal.ca/%7Eslacoste/"> Simon
  Lacoste-Julien </a> and <a href="http://mitliagkas.github.io">
  Ioannis Mitliagkas </a>. My current research focuses on the
theory and applications of convex and non-convex optimization in
large-scale machine learning and data science problems.<br>
<br>
I obtained my PhD from <a href="http://www.ed.ac.uk/">The
University of Edinburgh</a>, <a href="http://www.maths.ed.ac.uk/">School of Mathematics</a>. More specifically I was a member of the <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/research/ergo">Operational
  Research and Optimization Group (ERGO) </a> under the supervision
of Dr. <a href="https://richtarik.org/index.html"> Peter Richtarik</a>.
Prior to that, I spent 4 beautiful years in Athens as undergraduate
student in the Department of Mathematics at <a
  href="http://en.uoa.gr/"> National and Kapodistrian University of
  Athens</a> and 1 year as postgraduate student at Imperial College
London where I obtained an <a
href="http://www.imperial.ac.uk/computing/prospective-students/courses/pg/specialist-degrees/cms/">MSc
in      Computing (Computational Management Science)</a>. <br>
<br>
During fall of 2018, I was a research intern at <a
  href="https://research.fb.com/category/facebook-ai-research/">
  Facebook AI Research</a> in Montreal, Canada. I was working mainly
with <a href="https://research.fb.com/people/rabbat-mike/"> Dr.
  Mike Rabbat</a> on topics related to Distributed Non-Convex
Optimization Algorithms and Deep Learning. <br>
<br>
My research interests include (but are not limited to): <br>
Large Scale Optimization, Machine Learning, Randomized numerical
linear algebra, Convex Analysis, Randomized and Distributed
Algorithms. <br>
<br>
For more details please feel free to look my <a
  href="NicolasLoizouCV.pdf">CV</a>. <br>
<br>

<h2>Recent News</h2>
<hr>
<div id="" style="overflow-y:scroll; height:230px;">
  <ul>
    <ul>
      <li>
        <p> 01 Jun 2020: Two papers accepted to ICML 2020 (37th
          International Conference on Machine Learning):</p>
        <p><a href="https://arxiv.org/abs/2003.10422">A Unified
            Theory of Decentralized SGD with Changing Topology and
            Local Updates </a>.<br>
          joint work with Anastasia Koloskova, Sadra Boreiri, Martin
          Jaggi and Sebastian U. Stich. </p>
        <p> Stochastic Hamiltonian Gradient Methods for Smooth
          Games. <br>
          joint work with Hugo Berard, Alexia Jolicoeur-Martineau,
          Pascal Vincent, Simon Lacoste-Julien and Ioannis
          Mitliagkas.</p>
      </li>
      <li>
        <p> 23 Mar 2020: <b>New Paper out:</b> <a
            href="https://arxiv.org/abs/2003.10422"> A Unified
            Theory of Decentralized SGD with Changing Topology and
            Local Updates</a>, joint work with Anastasia Koloskova,
          Sadra Boreiri, Martin Jaggi and Sebastian U. Stich. </p>
      </li>
      <li>
        <p> 24 Feb 2020: <b>New Paper out:</b> <a
            href="https://arxiv.org/abs/2002.10542"> Stochastic
            Polyak Step-size for SGD: An Adaptive Learning Rate for
            Fast Convergence</a>, joint work with Sharan Vaswani,
          Issam Laradji and Simon Lacoste-Julien. </p>
      </li>
      <li>
        <p> 19 December 2019: I am delighted to be awarded the <a
            href="https://ivado.ca/en/ivado-scholarships/postdoctoral-scholarships/">
            IVADO Fellow Postdoctoral Scholarship </a>. <br>
          For more details on The Institute for Data Valorisation
          (IVADO) and its mission check out the <a
            href="https://ivado.ca/en/about-us/"> IVADO's website </a>
        </p>
      </li>
      <li>
        <p> 01 September 2019: Officially started as a Postdoctoral
          Fellow at <a href="https://mila.quebec/en/"> Mila -
            Quebec Artificial Intelligence Institute</a>. </p>
      </li>
      <li>
        <p> 28 June 2019: <b>Thesis Defense!</b> <br>
          I have successfully defended my PhD thesis "Randomized
          Iterative Methods for Linear Systems: Momentum,
          Inexactness and Gossip" today. A copy of the final version
          of the manuscript is available <a
            href="PhDThesis_Loizou2019.pdf">here</a> </p>
      </li>
      <li>
        <p> 20 May 2019: <b>New Paper out: </b> <a
            href="RandGossip.pdf"> Revisiting Randomized Gossip
            Algorithms: General Framework, Convergence Rates and
            Novel Block and Accelerated Protocols</a>. joint work
          with Peter Richtarik </p>
      </li>
      <li>
        <p> 22 Apr 2019: Two papers accepted to ICML 2019 (36th
          International Conference on Machine Learning):</p>
        <p><a href="https://arxiv.org/abs/1901.09401">SGD: General
            Analysis and Improved Rates</a>. joint work with Robert
          Mansel Gower, Xun Qian, Alibek Sailanbayev, Egor Shulgin
          and Peter Richtarik. </p>
        <p><a href="https://arxiv.org/abs/1811.10792">Stochastic
            Gradient Push for Distributed Deep Learning</a>. joint
          work with Mahmoud Assran, Nicolas Ballas and Michael
          Rabbat </p>
      </li>
      <li>
        <p> 07 Apr - 14 May 2019: I am visiting <a
            href="http://www.maths.ed.ac.uk/%7Erichtarik/index.html">
            Peter Richtarik</a> at <a
            href="https://vcc.kaust.edu.sa/Pages/Home.aspx"> KAUST </a>
        </p>
      </li>
      <li>
        <p> 19 Mar 2019: <b>New Paper out: </b> <a
            href="https://arxiv.org/abs/1903.07971"> Convergence
            Analysis of Inexact Randomized Iterative Methods</a>.
          joint work with Peter Richtarik. </p>
      </li>
      <li>
        <p> 01 Feb 2019: The paper <a
            href="https://arxiv.org/abs/1810.13084"> Provably
            Accelerated Randomized Gossip Algorithms</a>, coauthored
          with Mike Rabbat and Peter Richtarik, was accepted to the
          44th International Conference on Acoustics, Speech, and
          Signal Processing (ICASSP 2019). </p>
      </li>
      <li>
        <p> 27 Jan 2019: <b>New Paper out: </b> <a
            href="https://arxiv.org/abs/1901.09401"> SGD: General
            Analysis and Improved Rates</a>. joint work with Robert
          Mansel Gower, Xun Qian, Alibek Sailanbayev, Egor Shulgin
          and Peter Richtarik. </p>
      </li>
      <li>
        <p> 27 Nov 2018: <b>New Paper out: </b> <a
            href="https://arxiv.org/abs/1811.10792"> Stochastic
            Gradient Push for Distributed Deep Learning</a>. joint
          work with Mahmoud Assran, Nicolas Ballas and Michael
          Rabbat </p>
      </li>
      <li>
        <p> 31 Oct 2018: <b>New Paper out: </b> <a
            href="https://arxiv.org/abs/1810.13084"> Provably
            Accelerated Randomized Gossip Algorithms</a>. joint work
          with Michael Rabbat and Peter Richtarik </p>
      </li>
      <li>
        <p> 13 August - 17 Aug 2018: I am attending <a
            href="http://coral.ie.lehigh.edu/%7Emopta//">
            DIMACS/TRIPODS/MOPTA </a>, this week. On Tuesday I am
          presenting a poster and on Thursday I am giving a talk on
          ''Revisiting Randomized Gossip Algorithms". </p>
      </li>
      <li>
        <p> 09 Jul 2018: <b>New Paper out: </b> <a
            href="acc_gossip.pdf"> Accelerated Gossip via Stochastic
            Heavy Ball Method </a>. The paper is accepted to 56th
          Annual Allerton Conference on Communication, Control, and
          Computing, 2018 </p>
      </li>
      <li>
        <p> 01 Jul - 06 Jul 2018: I am attending <a
            href="https://ismp2018.sciencesconf.org/"> 23rd
            International Symposium on Mathematical Programming </a>
          this week. On Friday I am presenting our latest work:
          ''Convergence Analysis of Inexact Randomized Iterative
          Methods" </p>
      </li>
      <li>
        <p> 10 Apr - 15 May 2018: I am visiting <a
            href="http://mtakac.com/"> Martin Takac </a> and the <a
            href="http://optml.lehigh.edu/people/"> Optimization and
            Machine Learning Research Group at Lehigh University </a>
        </p>
      </li>
      <li>
        <p> 01 Feb - 21 Mar 2018: I am visiting <a
            href="http://www.maths.ed.ac.uk/%7Erichtarik/index.html">
            Peter Richtarik</a> at <a
            href="https://vcc.kaust.edu.sa/Pages/Home.aspx"> KAUST </a>
        </p>
      </li>
      <li>
        <p> 22 Dec 2017: <b>New Paper out: </b> <a
            href="https://arxiv.org/abs/1712.09677"> Momentum and
            Stochastic Momentum for Stochastic Gradient, Newton,
            Proximal Point and Subspace Descent Methods </a> </p>
      </li>
      <li>
        <p> 04-09 Dec 2017: I am attending <a
            href="https://nips.cc/Conferences/2017"> NIPS</a> this
          week. Our work <a href="https://arxiv.org/abs/1710.10737">
            Linearly convergent stochastic heavy ball method for
            minimizing generalization error </a> is presented at <a
            href="http://opt-ml.org/"> NIPS Workshop on Optimization
            for Machine Learning </a> </p>
      </li>
      <li>
        <p> 1-10 Oct 2017: I am visiting <a
            href="https://simons.berkeley.edu/"> Simons Institute
            for the Theory of Computing </a> at Berkeley,
          California. I am attending the workshop <a
            href="https://simons.berkeley.edu/workshops/optimization2017-2">
            Fast Iterative Methods in Optimization </a>. </p>
      </li>
    </ul>
  </ul>
</div>
</div>
</div>
</div>

 </body>
</html>
