<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Nicolas Loizou</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26204762-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>


  </head>


  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <ul class="menu">
            <li><a class="active" href="index.html">About Me</a></li>
            <li><a href="news.html"> News </a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="talks.html">Talks</a></li>
            <li><a href="bio.html">Bio</a></li>
            <li><a href="ProfServ.html">Professional Services</a></li>
            <li><a href="teaching.html">Teaching</a></li>
          </ul>
          <br>
          <script src="table_header.js"></script> </div>

      <div id="wrapper" class="main">
        <div id="content">

<br>
<br>
<br>
<br>
<br>
<b>Interested in working with me:</b><br> Please apply to the <a
    href=" https://engineering.jhu.edu/ams/academics/graduate-studies/graduate-admissions-process-and-criteria/">
    AMS graduate program </a>  and mention my name in your application. The deadline is December 15th, 2021. If you are already at JHU or have been admitted to JHU, feel free to send me an email.
    <br>
    <br>
    I am looking to recruit MSE &amp; Ph.D. students for Fall'22 in the following areas:
    <ul>
          <li> Optimization for machine learning and data science. </li>
          <li> Algorithms for differentiable/smooth games, adversarial formulations, variational inequalities.  </li>
          <li> Algorithms for collaborative learning (distributed, federated, and decentralized protocols).
          <li> Theory of deep learning.
          <li> Randomized numerical linear algebra (linear systems solvers, sketch and project methods).
          </ul>
    <br>
<h1>About Me</h1>
<hr> <br>
 I am an Assistant Professor in the <a
            href="https://engineering.jhu.edu/ams/">
            Department of Applied
Mathematics and Statistics</a> (AMS) and the <a
    href="https://www.minds.jhu.edu/about-minds-2/">
    Mathematical Institute for Data Science </a> (MINDS),  with a
secondary appointment in the <a
    href="https://www.cs.jhu.edu/">
    Department of Computer Science</a> at <a
            href="https://www.jhu.edu/">
            Johns Hopkins University</a>. My current research focuses on the
          theory and applications of convex and non-convex optimization in
          large-scale machine learning and data science problems.
    <br>
    <br>
Prior to this, I was an <a
    href="https://ivado.ca/en/ivado-scholarships/postdoctoral-scholarships/">
    IVADO</a> postdoctoral fellow at <a href="https://mila.quebec/en/">
  Mila - Quebec Artificial Intelligence Institute</a> and <a href="https://diro.umontreal.ca/english/home/">
DIRO, UdeM</a>, where I worked closely with <a
  href="http://www.iro.umontreal.ca/%7Eslacoste/"> Dr. Simon
  Lacoste-Julien </a> and <a href="http://mitliagkas.github.io">
  Dr. Ioannis Mitliagkas</a>. <br>
  I obtained my PhD from <a href="http://www.ed.ac.uk/">The
University of Edinburgh</a>, <a href="http://www.maths.ed.ac.uk/">School of Mathematics</a>. More specifically I was a member of the <a
href="https://www.maths.ed.ac.uk/school-of-mathematics/research/data-decisions/optimization-and-operational-research"> Optimization and Operational Research Group </a> under the supervision
of <a href="https://richtarik.org/index.html"> Dr. Peter Richtarik</a>.
Before that, I spent 4 beautiful years in Athens as undergraduate
student in the Department of Mathematics at <a
  href="http://en.uoa.gr/"> National and Kapodistrian University of
  Athens</a> and 1 year as postgraduate student at Imperial College
London where I obtained an <a
href="https://www.imperial.ac.uk/computing/prospective-students/pg/msc-specialist-degrees/cms/">MSc
in      Computing (Computational Management Science)</a>. <br>
During fall of 2018, I was a research intern at <a
  href="https://research.fb.com/category/facebook-ai-research/">
  Facebook AI Research</a> in Montreal, Canada. I was working mainly
with <a href="https://research.fb.com/people/rabbat-mike/"> Dr.
  Mike Rabbat</a> on topics related to Distributed Non-Convex
Optimization Algorithms and Deep Learning. <br>
<br>
My research interests include (but are not limited to): <br>
Large Scale Optimization, Machine Learning, Randomized numerical
linear algebra, Convex Analysis, Randomized and Distributed
Algorithms. <br>
<br>
For more details please feel free to look my <a
  href="Resources/NicolasLoizouCV.pdf">CV (updated October 2021)</a>. <br>
<br>

<h2>Selected Recent News</h2>
<hr>
(For the full list of news please check the <a href="./news.html"> News </a> tab.)

    <ul>
      <li>
        <p> 18 Jan 2022: Three papers were accepted to <b>AISTATS 2022</b> (25th International Conference on
Artificial Intelligence and Statistics):</p>
        <p><a href="https://arxiv.org/abs/2111.08611"> Stochastic Extragradient: General Analysis and Improved Rates</a>,<br>
          joint work with Eduard Gorbunov, Hugo Berard, and Gauthier Gidel. </p>
      <p><a href="https://arxiv.org/abs/2110.04261"> Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity</a> <br>
          joint work with Eduard Gorbunov, and Gauthier Gidel.</p>
          <p><a href="https://arxiv.org/abs/2107.00464"> On the Convergence of Stochastic Extragradient for Bilinear Games with Restarted Iteration Averaging</a> <br>
              joint work with Chris Junchi Li, Yaodong Yu, Gauthier Gidel, Yi Ma, Nicolas Le Roux, Michael I Jordan.</p>
      </li>
      <li>
        <p> 07 Jan 2022: I officially started as an Assistant Professor in the <a
                   href="https://engineering.jhu.edu/ams/">
                   Department of Applied
       Mathematics and Statistics</a> (AMS) and the <a
           href="https://www.minds.jhu.edu/about-minds-2/">
           Mathematical Institute for Data Science </a> (MINDS), at <a
                   href="https://www.jhu.edu/">
                   Johns Hopkins University</a>!!!
                 </li>
      <li>
        <p> 06 Dec 2021: I am attending <a href="https://neurips.cc/"> NeurIPS 2021, </a> this week. <br>
          I am presenting our work <a href="https://arxiv.org/abs/2107.00052"> Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity</a>.
      </li>
      <li>
        <p> 25 Oct 2021: I am attending <a href="http://meetings2.informs.org/wordpress/anaheim2021/"> 2021 INFORMS Annual Meeting</a> this week. <br>
          Today I am presenting our recent work <a href="https://arxiv.org/abs/2107.00052"> Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity</a> at the virtual session <a href="https://cattendee.abstractsonline.com/meeting/10390/search?query=%40VirtualSessions%7EVirtual+Sessions&ad=search-results-banner_sessions&currentPage=1&searchId=1311"> Recent Advances in Stochastic Gradient Algorithms</a>.
      </li>
      <li>
        <p> 05 Oct 2021: <b>2020 COAP Best Paper Award</b> <br> Our paper <a href="https://link.springer.com/article/10.1007/s10589-020-00220-z"> Momentum and
            Stochastic Momentum for Stochastic Gradient, Newton,
            Proximal Point and Subspace Descent Methods</a>, published in Computational Optimization and Applications (COAP) was voted by the editorial board as the best paper appearing in the journal in 2020!
      </li>
      <li>
        <p> 28 Sept 2021: Our paper <a href="https://arxiv.org/abs/2107.00052"> Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity</a>, joint work with Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas and Simon Lacoste-Julien,
          was accepted to <b>NeurIPS 2021</b> </p>
      </li>
      <li>
        <p> 29 Apr 2021: Our paper <a href="https://arxiv.org/abs/1905.08645">"Revisiting Randomized Gossip Algorithms: General Framework, Convergence Rates and Novel Block and Accelerated Protocols"</a>, joint work with Peter Richtarik,
          was accepted to
  <a href="https://ieeeittrans.ee.technion.ac.il/"> IEEE Transactions on Information Theory. </a> </p>
      </li>
      <li>
        <p> 22 Jan 2021: Two papers were accepted to <b>AISTATS 2021</b> (24th International Conference on
Artificial Intelligence and Statistics):</p>
        <p><a href="https://arxiv.org/abs/2002.10542">Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence </a><br>
          joint work with Sharan Vaswani, Issam Laradji and Simon Lacoste-Julien. </p>
      <p><a href="https://arxiv.org/abs/2006.10311">SGD for structured nonconvex functions: Learning rates, minibatching and interpolation </a> <br>
          joint work with Robert M. Gower and Othmane Sebbouh.</p>
      </li>
      <li>
        <p> 16 Oct 2020: I am delighted to be selected as the runner-up of the <a href="https://www.theorsociety.com/membership/awards-medals-and-scholarships/the-doctoral-award/"> OR Societyâ€™s Doctoral Award </a> for 2019.
This is an award for the "Most Distinguished Body of Research leading to the Award of a Doctorate in the field of Operational Research" in the United Kingdom. </li>
      <li>
        <p> 19 Aug 2020: Our paper <a href="https://arxiv.org/abs/1903.07971">"Convergence Analysis of Inexact Randomized Iterative Methods"</a>, joint work with Peter Richtarik,
          was accepted to
  <a href="https://www.siam.org/publications/journals/siam-journal-on-scientific-computing-sisc">SIAM Journal on Scientific Computing</a> (<b>SISC</b>) </p>
      </li>
      <li>
        <p> 19 Aug 2020: Our paper <a href="https://arxiv.org/abs/1712.09677">"Momentum and Stochastic Momentum for Stochastic Gradient, Newton, Proximal Point and Subspace Descent Methods
  "</a>, joint work with Peter Richtarik,
          was accepted to
  <a href="https://www.springer.com/journal/10589">Computational Optimization and Applications</a> (<b>COAP</b>) </p>
      </li>
      <li>
        <p> 20 Jun 2020: <b>New Paper out:</b> <a
            href="https://arxiv.org/abs/2006.11573"> Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization
  </a>, joint work with Ahmed Khaled, Othmane Sebbouh, Robert M. Gower and Peter RichtÃ¡rik. </p>
      </li>
      <li>
        <p> 18 Jun 2020: <b>New Paper out:</b> <a
            href="https://arxiv.org/abs/2006.10311"> SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation</a>, joint work with Robert M. Gower and Othmane Sebbouh. </p>
      </li>
      <li>
        <p> 01 Jun 2020: Two papers were accepted to <b>ICML 2020</b> (37th
          International Conference on Machine Learning):</p>
        <p><a href="https://arxiv.org/abs/2003.10422">A Unified
            Theory of Decentralized SGD with Changing Topology and
            Local Updates </a><br>
          joint work with Anastasia Koloskova, Sadra Boreiri, Martin
          Jaggi and Sebastian U. Stich. </p>
      <p><a href="https://arxiv.org/abs/2007.04202">Stochastic Hamiltonian Gradient Methods for Smooth
      Games </a> <br>
          joint work with Hugo Berard, Alexia Jolicoeur-Martineau,
          Pascal Vincent, Simon Lacoste-Julien and Ioannis
          Mitliagkas.</p>
      </li>
        <p> 23 Mar 2020: <b>New Paper out:</b> <a
            href="https://arxiv.org/abs/2003.10422"> A Unified
            Theory of Decentralized SGD with Changing Topology and
            Local Updates</a>, joint work with Anastasia Koloskova,
          Sadra Boreiri, Martin Jaggi and Sebastian U. Stich. </p>
      </li>
      <li>
        <p> 24 Feb 2020: <b>New Paper out:</b> <a
            href="https://arxiv.org/abs/2002.10542"> Stochastic
            Polyak Step-size for SGD: An Adaptive Learning Rate for
            Fast Convergence</a>, joint work with Sharan Vaswani,
          Issam Laradji and Simon Lacoste-Julien. </p>
      </li>
      <li>
        <p> 19 December 2019: I am delighted to be awarded the <a
            href="https://ivado.ca/en/ivado-scholarships/postdoctoral-scholarships/">
            IVADO Fellow Postdoctoral Scholarship </a>. <br>
          For more details on The Institute for Data Valorisation
          (IVADO) and its mission check out the <a
            href="https://ivado.ca/en/about-us/"> IVADO's website </a>
        </p>
      </li>
    </ul>
</div>
</div>
</div>

 </body>
</html>
